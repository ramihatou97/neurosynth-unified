diff --git a/src/synthesis/engine.py b/src/synthesis/engine.py
--- a/src/synthesis/engine.py
+++ b/src/synthesis/engine.py
@@ -1153,6 +1153,77 @@ class SynthesisEngine:

         return result

+    def compute_synthesis_score(self, result: SynthesisResult) -> Dict[str, float]:
+        """
+        Compute comprehensive quality score for synthesis output.
+
+        Scoring dimensions:
+        1. Content coverage (40%): Section completeness, word count
+        2. Source authority (25%): Authority tier weighting of sources
+        3. Visual integration (15%): Figure resolution rate, image relevance
+        4. Coherence (20%): Conflict count, gap coverage
+
+        Args:
+            result: SynthesisResult from synthesize()
+
+        Returns:
+            Dict with dimension scores and overall score (0.0-1.0)
+        """
+        scores = {}
+
+        # Dimension 1: Content Coverage (0.0-0.40)
+        section_count = len(result.sections)
+        expected_sections = len(TEMPLATE_SECTIONS.get(result.template_type, []))
+        section_coverage = min(1.0, section_count / max(expected_sections, 1))
+
+        word_count = result.total_words
+        min_words = TEMPLATE_REQUIREMENTS.get(result.template_type, {}).get("min_words", 2000)
+        max_words = TEMPLATE_REQUIREMENTS.get(result.template_type, {}).get("max_words", 8000)
+        word_coverage = min(1.0, word_count / min_words) if word_count < min_words else 1.0
+        word_penalty = 0.1 if word_count > max_words * 1.2 else 0.0  # Penalty for excessive length
+
+        scores["content_coverage"] = (section_coverage * 0.5 + word_coverage * 0.5 - word_penalty) * 0.40
+
+        # Dimension 2: Source Authority (0.0-0.25)
+        authority_scores = []
+        for ref in result.references:
+            # Parse authority tier from reference (default to tier 3)
+            auth_str = getattr(ref, 'authority', '') if hasattr(ref, 'authority') else ''
+            if 'rhoton' in auth_str.lower() or 'lawton' in auth_str.lower():
+                authority_scores.append(1.0)
+            elif 'textbook' in auth_str.lower() or 'atlas' in auth_str.lower():
+                authority_scores.append(0.9)
+            else:
+                authority_scores.append(0.7)
+
+        avg_authority = sum(authority_scores) / max(len(authority_scores), 1)
+        scores["source_authority"] = avg_authority * 0.25
+
+        # Dimension 3: Visual Integration (0.0-0.15)
+        figure_requests = len(result.figure_requests)
+        resolved_figures = len(result.resolved_figures)
+        resolution_rate = resolved_figures / max(figure_requests, 1) if figure_requests > 0 else 0.5
+
+        has_any_figures = result.total_figures > 0
+        figure_bonus = 0.05 if has_any_figures else 0.0
+
+        scores["visual_integration"] = (resolution_rate * 0.10 + figure_bonus)
+
+        # Dimension 4: Coherence (0.0-0.20)
+        conflict_count = result.conflict_count
+        conflict_penalty = min(0.15, conflict_count * 0.03)  # -3% per conflict, max -15%
+
+        gap_penalty = 0.0
+        if result.gap_report:
+            critical_gaps = result.gap_report.critical_gaps
+            gap_penalty = min(0.10, critical_gaps * 0.05)  # -5% per critical gap, max -10%
+
+        scores["coherence"] = max(0.0, 0.20 - conflict_penalty - gap_penalty)
+
+        # Overall score
+        scores["overall"] = sum(scores.values())
+        scores["grade"] = "A" if scores["overall"] >= 0.85 else "B" if scores["overall"] >= 0.70 else "C" if scores["overall"] >= 0.55 else "D"
+
+        return scores
+
     def _map_template_type(self, template_type: TemplateType) -> "GapTemplateType":
         """Map synthesis TemplateType to gap detection TemplateType."""
         if not GAP_DETECTION_AVAILABLE:
